{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp3269Uu303u"
      },
      "source": [
        "# **Mapping and Perception for an autonomous robot (0510-7951)**\n",
        "\n",
        "#Exercise 1:\n",
        "---\n",
        "**Part A**\n",
        "\n",
        "Experience with the KITTI dataset [**Robotic kit components**](https://www.cvlibs.net/datasets/kitti/) and [**pykitti tool**](https://pypi.org/project/pykitti/)\n",
        "\n",
        "**Part B**\n",
        "\n",
        "Robot mapping based on [**probability occupancy grid**](https://ieeexplore.ieee.org/document/8666170)\n",
        "\n",
        "**Part C**\n",
        "\n",
        "Sensor fusion and Road segemenation based on [**DeepLABv3+**](https://arxiv.org/abs/1703.06870) neural network\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "1. Fill in the code the \"TODO\" section\n",
        "\n",
        "2. **Please copy all the results to the report:**\n",
        "  - Outputs- Images, tables, scores,etc\n",
        "  - Performace, analysis and your explanations.\n",
        "  - Attach the completed notebook to the report package.\n",
        "3. Download and extract project data to your Google Drive\n",
        "\n",
        "  1.   Install Google Drive on your desktop.\n",
        "  2.   Save this notebook to your Google Drive by clicking `Save a copy in Drive` from the `File` menu.\n",
        "  3. Go to [**KITTI dataset**](http://www.cvlibs.net/datasets/kitti/) ,and download your specific records (chose \"sync\" recording)\n",
        "  5. Load the record to you goole drive account. Mount the recorcds to the notebook.\n",
        "  6. The git include example of records ('0117'), you can use it to learn how to arrange the format folders for pykitti toolbox.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "###REFERENCES\n",
        "\n",
        "[1] The full KITTI datased can be accessed here: http://www.cvlibs.net/datasets/kitti/.\n",
        "\n",
        "[2] KITTI Dataset paper: A. Geiger, P. Lenz, C. Stiller and R. Urtasun, \"Vision meets Robotics: The KITTI Dataset,\" International Journal of Robotics Research (IJRR), vol. 32, no. 11, pp. 1231-1237 2013.\n",
        "\n",
        "[3] Description of Occupancy Grid Map (OGM) estimation: Z. Luo, M. V. Mohrenschilt and S. Habibi, \"A probability occupancy grid based approach for real-time LiDAR ground segmentation,\" IEEE Transactions on Intelligent Transportation Systems, vol 21, no. 3, pp. 998–1010, Mar. 2020.\n",
        "\n",
        "[4] Paper of DeepLab v3+ for image segmentation: L. C. Chen, Y. Zhu, G. Apandreou, F. Schroff and H. Adam, “Encoder-decoder with atrous separable convolution for semantic image segmentation,” ECCV 2018 Lecture Notes in Computer Science, vol. 11211, pp. 833–851, 2018.\n",
        "\n",
        "[5] DeepLab v3+ paper via arXiv: https://arxiv.org/abs/1802.02611.\n",
        "\n",
        "[6] DeepLab v3+ repository: https://github.com/tensorflow/models/tree/master/research/deeplab.\n",
        "\n",
        "[7] This tutorial use pykitti module to load the KITTI dataset: https://github.com/utiasSTARS/pykitti."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO#\n",
        "\n",
        "# Name of student:\n",
        "# ID\n",
        "# Record number:"
      ],
      "metadata": {
        "id": "GjKEpi5RZ4dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJJQ3i9uUsnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6550a79-8799-4b10-90c1-f6e5fa8f9650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "## To import google drive, write this code in code section of colab and run it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iderI-dPJ7QJ"
      },
      "source": [
        "## PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lDKfTLg5WQo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/orfaig/course_ex1.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First- install pykitti\n",
        "!pip install pykitti"
      ],
      "metadata": {
        "id": "WcWQNRk1YLEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#second- install the other dependencies\n",
        "!pip install -r course_ex1/requirements.txt"
      ],
      "metadata": {
        "id": "KjvEqTEsw7iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBjbCBkk6gI1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pykitti\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymap3d as pm\n",
        "import pandas as pd\n",
        "#import getopt, os, sys, csv, numpy as np\n",
        "import open3d as o3d\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "YuMl7W2ZA6y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC0LVdNh66r6"
      },
      "outputs": [],
      "source": [
        "### Load KITTI Data\n",
        "\n",
        "# from my google drive (example)\n",
        "# basedir = 'gdrive/MyDrive/TAU/course/kitti'\n",
        "# date = '2011_09_26'\n",
        "# drive = '0117'\n",
        "\n",
        "#directly from colab\n",
        "basedir = 'course_ex1/raw_data/'\n",
        "date = '2011_09_26'\n",
        "drive = '0117'\n",
        "\n",
        "data = pykitti.raw(basedir, date, drive)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aO0SV94nwM9"
      },
      "outputs": [],
      "source": [
        "# *********************   LLA Coordinates  *********************\n",
        "fig_dir_path = os.getcwd() + \"/Results/\"\n",
        "if not os.path.exists(fig_dir_path):\n",
        "    os.makedirs(fig_dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYv70lTMYj5"
      },
      "source": [
        "#Part A: Geodetic coordinate system and the KITTI dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpful functions"
      ],
      "metadata": {
        "id": "1cRxPtQObeuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNLGBeT9ursS"
      },
      "outputs": [],
      "source": [
        "def unpack_params(dataset):\n",
        "    lons = [i.packet.lon for i in dataset.oxts]\n",
        "    lats = [i.packet.lat for i in dataset.oxts]\n",
        "    alts = [i.packet.alt for i in dataset.oxts]\n",
        "    num_sats = [i.packet.numsats for i in dataset.oxts]\n",
        "    pitches = [i.packet.pitch for i in dataset.oxts]\n",
        "    rolls = [i.packet.roll for i in dataset.oxts]\n",
        "    yaws = [i.packet.yaw for i in dataset.oxts]\n",
        "    times = dataset.timestamps\n",
        "    accs = [i.packet.pos_accuracy for i in dataset.oxts]\n",
        "    return lons, lats, alts, num_sats, accs, pitches, rolls, yaws, times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq8i9zSYusZg"
      },
      "outputs": [],
      "source": [
        "lons, lats, alts, num_satsa, accs, pitches, rolls, yaws, times = unpack_params(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.b)\n",
        "Print the trajectory on google maps using gmplot!"
      ],
      "metadata": {
        "id": "a5iTmQVrhY5U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40QPSO2zMuQO"
      },
      "outputs": [],
      "source": [
        "import gmplot \"\n",
        "\n",
        "#TODO -Print the trajectory on google maps (hint- use gmplot.GoogleMapPlotterp)\n",
        "\n",
        "# open the file Trajectory.html on chrome and see the trajectory on google maps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(1.c)"
      ],
      "metadata": {
        "id": "33Z-dPUSy3Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load vehicle velocities\n",
        "def load_vehicle_vel(data,idx,old_idx):\n",
        "  #calc differce time (total_seconds) per frames:\n",
        "  delta_t = #TODO\n",
        "\n",
        "  # read angular velocities: vf/s + vr/s (hint- vl is the opposite to vr)\n",
        "  packet = data.oxts[idx].packet\n",
        "  vf = #TODO (forawrd velocity)\n",
        "  vr = #TODO (angular velocity of the right side)\n",
        "\n",
        "  # calc angular velocities per frame: vf/frame + vr/frame\n",
        "  vfs = #TODO\n",
        "  vrs = #TODO\n",
        "\n",
        "  # calc yaw (substruct the initial yaw)\n",
        "  pose_y = #TODO\n",
        "  return (vfs,vrs,pose_y)"
      ],
      "metadata": {
        "id": "6xjEcnxadtlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R8QZC_ieoSe"
      },
      "outputs": [],
      "source": [
        "def Extarct_ENU_NED(lats,lons, alts):\n",
        "    \"\"\"\n",
        "\n",
        "  :LLA coordinates from pykitti.raw()\n",
        "  :return: ENU_coords_array,NED_coords_array\n",
        "  \"\"\"\n",
        "\n",
        "    #calc- trajectory in ENU (HINT- use pm.geodetic2enu)\n",
        "    #ENU_coords_array=\n",
        "\n",
        "    #calc- trajectory in NED (HINT- use pm.geodetic2ned)\n",
        "    #ENU_coords_array=\n",
        "\n",
        "    return ENU_coords_array,NED_coords_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting positions\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(18, 5))\n",
        "fig.suptitle('Trajectory on ENU')\n",
        "ax1.plot(ENU_coords_array[:, 0],ENU_coords_array[:, 1], color=\"blue\")\n",
        "ax1.set_title('ENU trajectory')\n",
        "ax1.set(xlabel='East[m]', ylabel='North[m]')\n",
        "ax1.grid()\n",
        "ax1.legend([\"ENU\"], loc='upper left')\n",
        "\n",
        "fig.suptitle('Trajectory on NED')\n",
        "ax2.plot(NED_coords_array[:, 0],NED_coords_array[:, 1], color=\"blue\")\n",
        "ax2.set_title('NED trajectory')\n",
        "ax2.set(xlabel='East[m]', ylabel='North[m]')\n",
        "ax2.grid()\n",
        "ax2.legend([\"NED\"], loc='upper left')\n",
        "\n",
        "fig.suptitle(\"velocity [m/s]\")\n",
        "ax3.plot(#TODO, color=\"blue\")\n",
        "ax3.set_title('velocity [m/s]')\n",
        "ax3.set(xlabel='frame', ylabel='right [m/s]')\n",
        "ax3.grid()\n",
        "\n",
        "\n",
        "# plotting  orientation\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12, 5))\n",
        "fig.suptitle(\"Roll Pitch and Yaw on ENU coordinates\")\n",
        "ax1.plot(roll/np.pi*180)\n",
        "ax1.plot(pitch/np.pi*180)\n",
        "ax1.set_title('Roll & Pitch Vs Frames')\n",
        "ax1.set(xlabel='Frame', ylabel='deg')\n",
        "ax1.legend([\"Roll\",\"Pitch\"],loc='upper left')\n",
        "ax1.grid()\n",
        "ax2.plot(yaw/np.pi*180,color='r')\n",
        "ax2.set_title('Yaw Vs Frames')\n",
        "ax2.set(xlabel='Frame', ylabel='deg')\n",
        "ax2.legend([\"Yaw\"], loc='upper left')\n",
        "ax2.grid()\n",
        "plt.savefig(fig_dir_path + \"/\" + \"Roll Pitch and Yaw on ENU coordinates.png\")"
      ],
      "metadata": {
        "id": "BhrK1SQlcQpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.d)"
      ],
      "metadata": {
        "id": "GpkcmPEia0uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#PlotSatellites Number\n",
        "\n",
        "def PlotSatellitesNumber(dataset):\n",
        "  \"\"\"\n",
        "  plotting and saving the satellites number\n",
        "  :param dataset: dataset from pykitti.raw()\n",
        "  :param data_sequence: the video number\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "\n",
        "  #TODO# (hint-use: dataset.oxts[i].packet.numsats)\n",
        "  plt.figure()\n",
        "  plt.title(\"Number of receiving satellites Vs Frames\")\n",
        "  plt.plot(satellites_num)\n",
        "  plt.xlabel(\"Frame\")\n",
        "  plt.ylabel(\"Number of receiving satellites\")\n",
        "  plt.grid()\n",
        "  plt.savefig(fig_dir_path+\"/\"+\"Number of receiving satellites Vs Frames.png\")"
      ],
      "metadata": {
        "id": "h6rBS-kiaz7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1.e)"
      ],
      "metadata": {
        "id": "TcAF4RXRiAzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Load point cloud data (LiDAR) and Image\n",
        "\n",
        "def load_data(data,idx):\n",
        "  ### Get the image data\n",
        "\n",
        "  img_raw=#TODO (hint- use get_cam2 in pykitti)\n",
        "\n",
        "  ### Get the LiDAR data (only x,y,z without the intensity) , (hint use get_velo )\n",
        "  lidar_raw= #TODO\n",
        "\n",
        "  return img_raw,lidar_raw"
      ],
      "metadata": {
        "id": "HfN3aLWfiAPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "#idx=#TODO\n",
        "img_raw,lidar_raw = load_data(data,idx)\n",
        "pc=lidar_raw.T\n",
        "### Visualize\n",
        "dataf = []\n",
        "x=pc[0,:];\n",
        "point_size = np.full(x.shape, 2)\n",
        "fig = px.scatter_3d(x=#TODO, y=#TODO, z=#TODO, size=point_size, opacity=1)\n",
        "dataf = [go.Scatter3d(x=#TODO, y=#TODO, z=#TODO, mode=\"markers\", marker=dict(size=2,color=pc[2,:],colorscale='Viridis'))]\n",
        "mega_centroid = np.average(pc, axis=1)\n",
        "mega_max = np.amax(pc, axis=1)\n",
        "mega_min = np.amin(pc, axis=1)\n",
        "lower_bound = mega_centroid - (np.amax(mega_max - mega_min) / 2)\n",
        "upper_bound = mega_centroid + (np.amax(mega_max - mega_min) / 2)\n",
        "\n",
        "show_grid_lines=True\n",
        "# Setup layout\n",
        "grid_lines_color = 'rgb(127, 127, 127)' if show_grid_lines else 'rgb(30, 30, 30)'\n",
        "layout = go.Layout(scene=dict(\n",
        "        xaxis=dict(nticks=8,\n",
        "                range=[lower_bound[0], upper_bound[0]],\n",
        "                showbackground=True,\n",
        "                backgroundcolor='rgb(30, 30, 30)',\n",
        "                gridcolor=grid_lines_color,\n",
        "                zerolinecolor=grid_lines_color),\n",
        "        yaxis=dict(nticks=8,\n",
        "                range=[lower_bound[1], upper_bound[1]],\n",
        "                showbackground=True,\n",
        "                backgroundcolor='rgb(30, 30, 30)',\n",
        "                gridcolor=grid_lines_color,\n",
        "                zerolinecolor=grid_lines_color),\n",
        "        zaxis=dict(nticks=8,\n",
        "                range=[lower_bound[2], upper_bound[2]],\n",
        "                showbackground=True,\n",
        "                backgroundcolor='rgb(30, 30, 30)',\n",
        "                gridcolor=grid_lines_color,\n",
        "                zerolinecolor=grid_lines_color),\n",
        "        xaxis_title=\"x (meters)\",\n",
        "        yaxis_title=\"y (meters)\",\n",
        "        zaxis_title=\"z (meters)\"\n",
        "    ),\n",
        "    margin=dict(r=10, l=10, b=10, t=10),\n",
        "    paper_bgcolor='rgb(30, 30, 30)',\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        color=grid_lines_color\n",
        "    ),\n",
        "    legend=dict(\n",
        "        font=dict(\n",
        "            family=\"Courier New, monospace\",\n",
        "            color='rgb(127, 127, 127)'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=dataf,layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YtbfhCpZ1680"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B: Probabilistic Occupancy Grid"
      ],
      "metadata": {
        "id": "EZXtOnG_ZOX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B.1.a)"
      ],
      "metadata": {
        "id": "DYEWUbAcfz_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Filter_PC(lidar_raw):\n",
        "\n",
        "  ### Only use LiDAR points that are below the sensor (set threshold)\n",
        "  ### Only use LiDAR points that are at least 2.5 m away from the sensor\n",
        "\n",
        "  lidar_raw_obstacles= #TODO (all the point clouds above ~30 cm off the ground- obstacles)\n",
        "\n",
        "  lidar_raw_road= #TODO (free path under the threshold), ### use same threshold to extarct the point clounds of the estimate road\n",
        "\n",
        "  return lidar_raw_obstacles,lidar_raw_road\n"
      ],
      "metadata": {
        "id": "dTaGog1pft84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=1\n",
        "img_raw,lidar_raw = load_data(data,idx)\n",
        "lidar_raw,lidar_raw_road=Filter_PC(lidar_raw)\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(2,1,figsize=(15,12))\n",
        "axs[0].imshow( #TODO)\n",
        "axs[1].scatter(lidar_raw[:,0],lidar_raw[:,1],c=-lidar_raw[:,2],marker='.')\n",
        "axs[1].scatter(lidar_raw_road[:,0],lidar_raw_road[:,1],c='b',marker='.')\n",
        "axs[1].scatter(0,0,c='r',marker='x')\n",
        "axs[1].set_title(\"Naive method for driveable path segmentation\")\n",
        "axs[1].set_xlim( #TODO,#TODO)\n",
        "axs[1].set_ylim(#TODO,#TODO)\n",
        "axs[1].axis('scaled')\n",
        "axs[1].grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Pq5ZZWNfy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(B.1.b)"
      ],
      "metadata": {
        "id": "Pc2YtGh9dC_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iYW3huTqMdu"
      },
      "source": [
        "The Occupancy Grid Map (OGM) in this tutorial is estimated with the procedure described in [3]. OGM is a grid-based (image-like) map where each of its cell/pixel contains probability of that cell occupied by any obstacle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTDL31Mp_tk"
      },
      "source": [
        "To update the OGM with the most recent measurement, the LiDAR points need to be converted to the similar grid format. This grid is called the scan grid (SG).\n",
        "\n",
        "Take a look at the figure of a SG below. In the figure, the purple point is a sample of the LiDAR measurements. There are three conditions to fill the SG.\n",
        "1. Black cells: The cells around the point (cells that are radially located at +-ALPHA and angularly located at +-BHETA from the point) are given probability = 0.8 which means that they are likely to be occupied.\n",
        "2. White cells: The cells that are located between the origin/sensor and the measured point are given probability = 0.2 which means that they are likely to be free.\n",
        "3. Gray cells: The other cells, including the ones behind the points, are given probability = 0.5 which means that we can't infer whether they are free or occupied.\n",
        "\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/sg_gen.png\" width=300px></center>\n",
        "</br>\n",
        "\n",
        "\n",
        "The efficient implementation of the SG creation is not straightforward. In this tutorial, we create the SG in spherical coordinate first. After the spherical SG is filled, then it is converted to cartesian coordinate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWU3LkB2XyE7"
      },
      "source": [
        "There are several parameters that need to be defined:\n",
        "- ALPHA - The radial resolution when converting LiDAR data to grid map in m (explained later).\n",
        "- BHETA - The angular resolution when converting LiDAR data to grid map in radian (explained later) .\n",
        "- RESOLUTION - The resolution of the grid map in meter.\n",
        "- MAX_RANGE - Maximum range of LiDAR points that will be converted to grid map.\n",
        "- MAP_WIDTH - Width of the map from side to side in meter.\n",
        "- SPHERICAL2CARTESIAN_BIAS - An adjustment needed due to some errors when converting the spherical grid map to the cartesian grid map."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map Initialization"
      ],
      "metadata": {
        "id": "ZbncWyCNjbSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OGM is initialized with probability of all of its cells = 0.5"
      ],
      "metadata": {
        "id": "76OU5JlCjetN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Some derived parameters\n",
        "# OOR_MASK = A mask to filter out measurements that are out of MAX_RANGE\n",
        "MAP_SIZE_X = int(MAP_WIDTH/RESOLUTION)\n",
        "MAP_SIZE_Y = int(MAP_WIDTH/RESOLUTION)\n",
        "xarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "yarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "MAP_XX, MAP_YY = np.meshgrid(xarr, -yarr)\n",
        "rgrid = np.sqrt(np.add(np.square(MAP_XX),np.square(MAP_YY)))\n",
        "OOR_MASK = rgrid >= MAX_RANGE\n",
        "\n",
        "### Initialize OGM\n",
        "ogm_time_0 = np.ones((MAP_SIZE_Y,MAP_SIZE_X)) * #TODO\n",
        "\n",
        "### Visualize\n",
        "# Yes, it's still empty\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(ogm_time_0,cmap='gray',vmax=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nVN9CDNqjXIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOONBcjwqrli"
      },
      "outputs": [],
      "source": [
        "ALPHA = #TODO\n",
        "BHETA = #TODO*np.pi/180\n",
        "RESOLUTION = #TODO\n",
        "MAX_RANGE = #TODO\n",
        "MAP_WIDTH = #TODO\n",
        "SPHERICAL2CARTESIAN_BIAS = #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of a grid on a single scan:"
      ],
      "metadata": {
        "id": "ukuo9umBejnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saLBELizrISi"
      },
      "outputs": [],
      "source": [
        "def generate_measurement_ogm(lidar_in,ogm_shape):\n",
        "  ### Calculate the position of LiDAR points in spherical coordinate\n",
        "  ### build zeros matrix in size 2 for range and angular\n",
        "  rphi_meas = np.zeros((lidar_in.shape[0],2))\n",
        "  ### set the distance of x,y in col 1. use ALPH\n",
        "  rphi_meas[:,1] = #TODO\n",
        "  ### set the angular in col 0. use arctan2 and don't forget to add pi, and to use BHETA\n",
        "  rphi_meas[:,0] = #TODO\n",
        "\n",
        "   ### unique repatative points in same line of sight\n",
        "  rphi_meas =  #TODO\n",
        "\n",
        "  ### Set the cells below the maximum range to zero\n",
        "  rphi_meas = #TODO\n",
        "\n",
        "  ### set zero the cells that below the max angular\n",
        "  rphi_meas = #TODO\n",
        "\n",
        "  ### Initalize the spherical scan grid size\n",
        "  sg_ang_bin = int(2*np.pi/BHETA)\n",
        "  sg_rng_bin = int(MAX_RANGE/ALPHA)\n",
        "  scan_grid = np.ones((#TODO,#TODO))\n",
        "\n",
        "  # Initiation (Condition 1)\n",
        "  #Initalize spherical probability #(hint- unknown probability value)\n",
        "  #TODO\n",
        "\n",
        "  # Condition 2 - set the occupied probabilty value in the occupied cells\n",
        "  #TODO\n",
        "\n",
        "  # Condition 3  set the free probabilty score to drivable path and to free line of sight w/o obstacles.\n",
        "  for ang in range(sg_ang_bin):\n",
        "    ang_arr = rphi_meas[rphi_meas[:,0]==ang,1]\n",
        "    if len(ang_arr) == 0: #case a- line of sight is clear\n",
        "      #TODO\n",
        "    else:                 #case b- The line of sight is blocked, but the cells to the first occupied cell are clear.\n",
        "      #TODO\n",
        "  ### Convert the spherical scan grid to the cartesian one\n",
        "  ogm_sz = (ogm_shape[1],ogm_shape[0])\n",
        "  ogm_cen = (int(ogm_shape[1]/2),int(ogm_shape[0]/2))\n",
        "  radius = (MAX_RANGE/RESOLUTION) + SPHERICAL2CARTESIAN_BIAS\n",
        "\n",
        "  #use warpPolar to convert the polar map to cartesiaan (hint- use warpPolar, inputs: scan_grid,ogm shape ,ogm center ,range ,cv2.WARP_INVERSE_MAP)\n",
        "  return ogm_step"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display two single maps"
      ],
      "metadata": {
        "id": "iAa7YBNcjuGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx=0\n",
        "img_raw1,lidar_raw1 = load_data(data,idx)\n",
        "lidar_raw1,lidar_raw_road=Filter_PC(lidar_raw1)\n",
        "ogm_time_1 = generate_measurement_ogm(lidar_raw1,ogm_time_0.shape)\n",
        "\n",
        "idx2=1\n",
        "img_raw2,lidar_raw2 = load_data(data,idx2)\n",
        "lidar_raw2,lidar_raw_road=Filter_PC(lidar_raw2)\n",
        "ogm_time_2 = generate_measurement_ogm(lidar_raw2,ogm_time_0.shape)\n",
        "\n",
        "# ### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(((1-ogm_time_1)*255).astype(np.uint8))\n",
        "axs[1].imshow(img_raw1)\n",
        "plt.show()\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(2,2,figsize=(20,10))\n",
        "axs[0][0].imshow(((1-ogm_time_1)*255).astype(np.uint8))\n",
        "axs[0][0].set_title(\"frame 0\")\n",
        "axs[1][0].imshow(img_raw)\n",
        "axs[0][1].imshow(((1-ogm_time_2)*255).astype(np.uint8))\n",
        "axs[0][1].set_title(\"frame 1\")\n",
        "axs[1][1].imshow(img_raw2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-bBeYjfVi_Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B.1.c)"
      ],
      "metadata": {
        "id": "U3mKdc6khCnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try First Update"
      ],
      "metadata": {
        "id": "9VQdc8VUYnEw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6PtgspxxPGW"
      },
      "source": [
        "The OGM is updated by the recent scan grid using Bayesian update. According the derivation in the paper, if we initiate the OGM as empty grid, the $i$-th cell of OGM can be updated with this simple formula\n",
        "\n",
        "\\begin{align}\n",
        "L_{i,t} = L_{i,t-1} + L^{SG}_{i,t-1}\n",
        "\\end{align}\n",
        "\n",
        "$L_{i,t}$ is the [logit](https://en.wikipedia.org/wiki/Logit) of the $i$-th cell of the updated OGM.\n",
        "\n",
        "$L_{i,t-1}$ is the logit of the $i$-th cell of the previous OGM.\n",
        "\n",
        "$L^{SG}_{i,t-1}$ is the logit of the $i$-th cell of the scan grid that are generated from the latest LiDAR points.  \n",
        "\n",
        "Then, the usable OGM can be found by calculating the inverse-logit of $L_{i,t}$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Calculate the logit function\n",
        "def logit(m):\n",
        "  return #TODO\n",
        "\n",
        "### Calculate the inverse logit function\n",
        "def inverse_logit(m):\n",
        "  return #TODO\n",
        "\n",
        "### Update the prior OGM with the scan grid (new_ogm)\n",
        "def update_ogm(prior_ogm,new_ogm):\n",
        "  ## a.  calc logit map\n",
        "  logit_map =#TODO\n",
        "  ## b.  inverse to probability space\n",
        "  out_ogm = #TODO\n",
        "  ## c. apply saturation\n",
        "  #TODO\n",
        "  return out_ogm\n",
        "\n",
        "ogm_updated_1 = update_ogm(ogm_time_1,ogm_time_0)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(((1-ogm_updated_1)*255).astype(np.uint8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_-AlVJ7uYtw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Visualize Point cloud, probabilty and logit space\n",
        "fig,axs = plt.subplots(1,3,figsize=(20,10))\n",
        "axs[0].scatter(lidar_raw[:,0],lidar_raw[:,1],c=lidar_raw[:,2],marker='.',s=0.1)\n",
        "axs[0].scatter(0,0,c='r',marker='x')\n",
        "axs[0].set_title(\"Point cloud (Z>TH)\")\n",
        "axs[0].set_xlim(-80,80)\n",
        "axs[0].set_ylim(-80,80)\n",
        "axs[0].axis('scaled')\n",
        "axs[0].set_ylabel('y')\n",
        "axs[0].set_xlabel('x (heading')\n",
        "axs[0].grid()\n",
        "colorbar=axs[1].imshow(ogm_step1)\n",
        "fig.colorbar(colorbar,ax=axs[1],fraction=0.046, pad=0.1)\n",
        "axs[1].set_title(\"scan grid:\\n Probability represnetation\")\n",
        "axs[2].imshow(logit(ogm_step1))\n",
        "colorbar=axs[2].imshow(logit(ogm_step1),cmap='seismic')\n",
        "fig.colorbar(colorbar,ax=axs[2],fraction=0.046, pad=0.1)\n",
        "axs[2].set_title(\"scan grid:\\n Logit represnetation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L_QG7SuqE6Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/PC_prob_logit.png\" width=1200px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "9S5c3b2YPEsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B.1.d)"
      ],
      "metadata": {
        "id": "X8KFET3Lg8Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shift the OGM!"
      ],
      "metadata": {
        "id": "wOHqk_PjbLEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shift the OGM according to the next pose"
      ],
      "metadata": {
        "id": "FQFlQzt1ZGCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift the map according the vehicle's poses\n",
        "def shift_pose_ogm(ogm, init, fin):\n",
        "  ogm_o = ogm.copy()\n",
        "  trs_m = np.array([[init[0]],[init[1]]])\n",
        "  delta_theta = (fin[2] - init[2])\n",
        "  delta = np.array([trs_m[1,0]/RESOLUTION,trs_m[0,0]/RESOLUTION,0])\n",
        "\n",
        "  M = np.array([[1,0,-delta[0]],[0,1,delta[1]]])\n",
        "  dst = cv2.warpAffine(ogm_o,M,(ogm_o.shape[1],ogm_o.shape[0]),borderValue=0.5)\n",
        "  M = cv2.getRotationMatrix2D((ogm_o.shape[1]/2+0.5,ogm_o.shape[0]/2+0.5),-delta_theta/np.pi *180 ,1) #Angle of Rotation. Angle is positive for anti-clockwise and negative for clockwise\n",
        "  dst = cv2.warpAffine(dst,M,(ogm_o.shape[1],ogm_o.shape[0]),borderValue=0.5)\n",
        "  return dst"
      ],
      "metadata": {
        "id": "TEj6Xq_5bb8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pose of two frames - frame 1 and frame 2\n",
        "pose1 = load_vehicle_vel(data,idx,idx)\n",
        "pose2 = load_vehicle_vel(data,idx1,idx)"
      ],
      "metadata": {
        "id": "t9blZC3ubcD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shift_ogm_time_1 = shift_pose_ogm(ogm_time_1,pose1,pose2)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(((1-ogm_time_1)*255).astype(np.uint8))\n",
        "axs[0].set_title('Before')\n",
        "axs[1].imshow(((1-shift_ogm_time_1)*255).astype(np.uint8))\n",
        "axs[1].set_title('After')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Otmhm2Xbbj8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Update next frame"
      ],
      "metadata": {
        "id": "kgQlBTCV4MEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ogm_updated_2 = update_ogm(shift_ogm_time_1,ogm_time_2)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(20,10))\n",
        "axs[0].imshow(((1-ogm_time_2)*255).astype(np.uint8))\n",
        "axs[0].set_title(\"frame 2\")\n",
        "axs[1].imshow(((1-ogm_updated_2)*255).astype(np.uint8))\n",
        "axs[1].set_title(\"frame 1+2\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KvVuPtrQbmS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Robot mapping!"
      ],
      "metadata": {
        "id": "RcVJSGd6d66M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(B.2)"
      ],
      "metadata": {
        "id": "mcDP89aMde7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "KkSEoPVNzmh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Transform the point cloud to INS location.\n",
        "2.   Align the point cloud based on IMU inputs."
      ],
      "metadata": {
        "id": "KB3GQItPzrpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdSDeDLNy-14"
      },
      "outputs": [],
      "source": [
        "def rotx(t):\n",
        "    \"\"\"Rotation about the x-axis.\"\"\"\n",
        "    return #TODO\n",
        "\n",
        "def roty(t):\n",
        "    \"\"\"Rotation about the y-axis.\"\"\"\n",
        "    return #TODO\n",
        "\n",
        "\n",
        "def rotz(t):\n",
        "    \"\"\"Rotation about the z-axis.\"\"\"\n",
        "    return #TODO\n",
        "\n",
        "velo_to_imu_calibration = #TODO (hint- use inverse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd1qftLc1iPU"
      },
      "outputs": [],
      "source": [
        "def transform_from_rot_trans(R, t):\n",
        "    \"\"\"Transforation matrix from rotation matrix and translation vector.\"\"\"\n",
        "    R = R.reshape(3, 3)\n",
        "    t = t.reshape(3, 1)\n",
        "    return np.vstack((np.hstack([R, t]), [0, 0, 0, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4L5_nHBxw24"
      },
      "source": [
        "##Repeat the occupancy map process over all your scans/frames!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXCX-fL7xOd3"
      },
      "outputs": [],
      "source": [
        "### Initialize OGM\n",
        "Occupancy_map = np.ones((MAP_SIZE_Y,MAP_SIZE_X)) * 0.5\n",
        "\n",
        "for idx_frame in range(#TODO):\n",
        "\n",
        "  # Load data\n",
        "  #TODO\n",
        "\n",
        "  # filter point cloud of the obstacles! (height above threshould)\n",
        "  #TODO\n",
        "\n",
        "  #Transfrom point cloud to baselink (imu)\n",
        "  #TODO\n",
        "\n",
        "  #ego sphere (apply the inverse of roll and pitch on the point cloud)\n",
        "  #use roll and pitch matrix\n",
        "  #use data.oxts.packet.roll and data.oxts.packet.pitch\n",
        "  #TODO\n",
        "\n",
        "\n",
        "  x_points = lidar_raw[:, 0]\n",
        "  y_points = lidar_raw[:, 1]\n",
        "  z_points = lidar_raw[:, 2]\n",
        "\n",
        "  velo=np.vstack((x_points, y_points,z_points )).T\n",
        "\n",
        "  if (idx_frame ==0):\n",
        "    pose_prev = (0,0,0)\n",
        "    prev_indx=idx_frame\n",
        "  else:\n",
        "\n",
        "    prev_indx=idx_frame-1\n",
        "\n",
        "  #Occupancy map\n",
        "  #pose = #TODO         (hint- use load_vehicle_vel )\n",
        "  #shifted_ogm =#TODO   (hint- use shift_pose_ogm)\n",
        "  #ogm_step =#TODO      (hint- generate_measurement_ogm)\n",
        "  #Occupancy_map =#TODO (hint- update_ogm)\n",
        "\n",
        "\n",
        "  pose_prev=pose\n",
        "\n",
        "  fig,axs = plt.subplots(2,2,figsize=(15,12))\n",
        "  fig1 = plt.gcf()\n",
        "  axs[0,0].imshow(img_raw)\n",
        "  axs[0,0].set_title(\"Occupancy Map At Frame Number={:03}\".format(idx_frame))\n",
        "  axs[0,0].axis('scaled')\n",
        "  axs[1,1].imshow(Occupancy_map)\n",
        "  axs[1,1].scatter(500,500,c='r',marker='x')\n",
        "  axs[1,1].axis('scaled')\n",
        "  axs[1,1].set_title(\"Occupancy Map\")\n",
        "  axs[1,0].imshow(ogm_step)\n",
        "  axs[1,0].scatter(500,500,c='r',marker='x')\n",
        "  axs[1,0].set_title(\"Hitmap (scan grid)\")\n",
        "  axs[0,1].scatter(-lidar_raw[:,1],lidar_raw[:,0],c=lidar_raw[:,2],marker='.')\n",
        "  axs[0,1].scatter(lidar_raw_road[:,1],lidar_raw_road[:,0],c='b',marker='.',s=0.1)\n",
        "  axs[0,1].scatter(0,0,c='r',marker='x')\n",
        "  axs[0,1].grid()\n",
        "  axs[0,1].axis('scaled')\n",
        "  axs[0,1].set_title(\"Naive method for road segmenation\")\n",
        "  axs[0,1].set_xlim(-80,80)\n",
        "  axs[0,1].set_ylim(-80,80)\n",
        "  plt.show()\n",
        "  plt.draw()\n",
        "  #fig1.savefig(\"%s/occupancy_map_frame_{:03}.png\".format(idx_frame) % '/content/gdrive/MyDrive/Exp1/', dpi=100 )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/naive_method_frame_3.png\" width=1200px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "4VVdd2AqOJZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B.2.g)"
      ],
      "metadata": {
        "id": "D62ETDvshZ5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply threshoulds on the occupancy map:\n",
        "The segmented maps should describe 3 types of cells: free, occluded and unknown."
      ],
      "metadata": {
        "id": "ftwVsmPV2Sq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_for_free=#TODO\n",
        "threshold_for_Occupied=#TODO\n",
        "\n",
        "### Visualize Segmented_occupancy_map\n",
        "fig,axs = plt.subplots(1,3,figsize=(20,10))\n",
        "axs[0].imshow(ogm_step)\n",
        "colorbar=axs[0].imshow(ogm_step)\n",
        "fig.colorbar(colorbar,ax=axs[0],fraction=0.046, pad=0.1)\n",
        "axs[0].set_title(\"scan grid\")\n",
        "\n",
        "\n",
        "colorbar=axs[1].imshow(Occupancy_map)\n",
        "fig.colorbar(colorbar,ax=axs[1],fraction=0.046, pad=0.1)\n",
        "axs[1].set_title(\"Occupancy_map\")\n",
        "axs[1].axis('scaled')\n",
        "axs[1].imshow(Occupancy_map)\n",
        "\n",
        "## segmented map!\n",
        "\n",
        "Occupancy_map_final=np.ones((MAP_SIZE_Y,MAP_SIZE_X)) * #TODO\n",
        "Occupancy_map_final[Occupancy_map < threshold_for_free]=#TODO\n",
        "Occupancy_map_final[Occupancy_map > threshold_for_Occupied]=#TODO\n",
        "\n",
        "axs[2].imshow(Occupancy_map_final)\n",
        "colorbar=axs[2].imshow(Occupancy_map_final,cmap='PRGn')\n",
        "fig.colorbar(colorbar,ax=axs[2],fraction=0.046, pad=0.1)\n",
        "axs[2].set_title(\"Segmented_occupancy_map\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wr6_51MKuG8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/segmented_map.png\" width=1200px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "niOkT9jlOeSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.1)"
      ],
      "metadata": {
        "id": "tbZ2FP1JheF4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDiDJTa7J-fB"
      },
      "source": [
        "# Part C: Sensor fusion and Road segemenation\n",
        "\n",
        "The goal of the perception system is to extract the information about the round where the vehicle is operating on. This road information will be used to filter out LiDAR points that hit the road so that it can be used for mapping purpose. To extract the information about where the road is, we use deep learning-based image segmentation technique that will be applied to the camera image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQYbI6wAbdv"
      },
      "source": [
        "## Get Callibration Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoBXqa6l2u8a"
      },
      "source": [
        "Two callibrated parameters that we need:\n",
        "- LiDAR to camera extrinsic matrix - The matrix (4x4) that will be used to transform the LiDAR points to the camera 3D coordinate frame.\n",
        "-  Camera intrinsic matrix - The mastrix (3x3) that will be used to calculate the coordinate of pixels that representat 3D points in camera coordinate.\n",
        "\n",
        "The calibrated parameters are already provided in the dataset. For details, please take a look at [this explanation](https://www.mathworks.com/help/vision/ug/camera-calibration.html) from Mathworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-24HbwND-fbH"
      },
      "outputs": [],
      "source": [
        "### Retrieve the provided calibration data\n",
        "lidar2cam_extrinsic = #TODO\n",
        "camera_intrinsic =  #TODO\n",
        "\n",
        "print('Lidar to camera extrinsic matrix: ')\n",
        "print(lidar2cam_extrinsic)\n",
        "print()\n",
        "print('Camera intrinsic matrix: ')\n",
        "print(camera_intrinsic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2sVVXDiAfw5"
      },
      "source": [
        "## Load Camera and LiDAR Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTycCXXRACXM"
      },
      "source": [
        "Coordinate system of\n",
        "- Camera = x: right, y: down, z: forward\n",
        "- LiDAR = x: forward, y: left, z: up\n",
        "\n",
        "In this tutorial, camera coordinate system is used. Therefore, the LiDAR points need to be transformed to the camera coordinate frame. Use the LiDAR to camera extrinsic matrix!\n",
        "\n",
        "The extrinsic matrix can be written as $\\begin{bmatrix} R|t \\end{bmatrix}$, a combinarion of a rotation matrix $R$ and a translation vector $t$. Given that the point in LiDAR coordinate is $(X_L,Y_L,Z_L)$, its coordinate in camera 3D frame, $(X_C,Y_C,Z_C)$ is\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "X_C \\\\\n",
        "Y_C \\\\\n",
        "Z_C \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix} R|t \\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "X_L \\\\\n",
        "Y_L \\\\\n",
        "Z_L \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kacf9ugO_XGU"
      },
      "outputs": [],
      "source": [
        "def filterLidarbelowsensor(lidar_raw):\n",
        "\n",
        "  ### Only use LiDAR points that are at least 2.5 m away (hint udr norm to find the distance)\n",
        "  #TODO\n",
        "\n",
        "   ### find LiDAR points that are below the sensor (z=0)\n",
        "  #TODO\n",
        "\n",
        "  return lidar_raw\n",
        "\n",
        "### Transform the LiDAR points into camera coordinate\n",
        "def transform_coordinate(lidar_points,extrinsic_matrix):\n",
        "  inp = lidar_points.copy()\n",
        "  #TODO (multiply extrinsic_matrix with lidar_points )\n",
        "  return inp[:,:3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRYSij3xc8AM"
      },
      "outputs": [],
      "source": [
        "idx=0\n",
        "idx1=1\n",
        "#load the data (hint- use load data)\n",
        "img_raw,lidar_raw = #TODO\n",
        "\n",
        "#filter Lidar below sensor (hint-  filterLidarbelowsensor)\n",
        "lidar_raw_filter=#TODO\n",
        "\n",
        "#transform point cloud to camera coordinate\n",
        "lidar_raw_cam = transform_coordinate(lidar_raw_filter,lidar2cam_extrinsic)\n",
        "\n",
        "# same as indx\n",
        "lidar_raw_cam1=#TODO\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(2,2,figsize=(15,12))\n",
        "axs[0,0].imshow(img_raw)\n",
        "axs[0,1].scatter(lidar_raw_cam[:,0],lidar_raw_cam[:,2],c=-lidar_raw_cam[:,1],marker='.')\n",
        "axs[0,1].scatter(0,0,c='r',marker='x')\n",
        "axs[0,1].set_title(\"point cloud includes the road!\")\n",
        "axs[0,1].axis('scaled')\n",
        "axs[1,0].imshow(img_raw1)\n",
        "axs[1,1].scatter(lidar_raw_cam1[:,0],lidar_raw_cam1[:,2],c=-lidar_raw_cam1[:,1],marker='.')\n",
        "axs[1,1].scatter(0,0,c='r',marker='x')\n",
        "axs[1,1].axis('scaled')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QTNtHYfCs2e"
      },
      "source": [
        "## Project the LiDAR Points to the Camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytg6gknC6nYY"
      },
      "source": [
        "To transfer the road information from image to LiDAR, we need to project the LiDAR points to the camera image. Use the camera intrinsic matrix!\n",
        "\n",
        "The camera intrinsic matrix is denoted as $K$. The coordinate of pixel, $(u,v)$, that represents a point in 3D space, $(X_C,Y_C,Z_C)$, in image frame can be calculated with\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "u\\times w \\\\\n",
        "v\\times w \\\\\n",
        "w \\\\\n",
        "\\end{bmatrix}=\n",
        "K\n",
        "\\begin{bmatrix}\n",
        "X_C \\\\\n",
        "Y_C \\\\\n",
        "Z_C \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://docs.opencv.org/4.5.0/pinhole_camera_model.png\" width=400px></center>\n",
        "</br>\n",
        "\n",
        "Sumber gambar: [OpenCV Docs](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.1.b)"
      ],
      "metadata": {
        "id": "PtJUGnhch_tO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwSf1GLfCr1T"
      },
      "outputs": [],
      "source": [
        "def project_lidar2cam(lidar_raw_cam,camera_intrinsic,img_raw_size):\n",
        "  #Inputs:\n",
        "  #lidar_in_cam: lidar raw data in camera axis coordiantes: x: right, y: down, z: forward\n",
        "  #camera_intrinsic,img_raw_size\n",
        "\n",
        "  ### 1. Filter out data behind the camera (hint- take only the forward points (z>0 in camera coordinates) and not the backward points)\n",
        "  #TODO\n",
        "  lidar_raw_cam = np.concatenate((lidar_raw_cam,np.ones((lidar_in_cam.shape[0],1))),axis=1)\n",
        "  lidar_in_cam = lidar_raw_cam[lidar_raw_cam[:,2]>0]\n",
        "\n",
        "  ### 2. Project points to the image\n",
        "  # hint-1 (use camera_intrinsic )\n",
        "  # hint-2 convert wx,wy and w to image coordinates(\"lidar 2d\"): u,v (divide u=x/w and v=y/w )\n",
        "  #TODO\n",
        "\n",
        "  lidar_2d = #TODO\n",
        "  lidar_2d = #TODO\n",
        "  lidar_2d = lidar_2d.astype(int)\n",
        "\n",
        "  ### 3. Filter out points that are outside image frame\n",
        "  ## (hint- use maskH and maskv, see exaplation below)\n",
        "  #TODO\n",
        "  lidar_2d = #TODO\n",
        "  lidar_in_cam_img = #TODO\n",
        "\n",
        "  return #lidar_2d (image coordinates),lidar_in_cam_img[:,:3] (camera coordinates, but only the PC in the image FOV)\n",
        "\n",
        "img_raw_size = img_raw.shape\n",
        "\n",
        "lidar_2d,lidar_in_cam_img = project_lidar2cam(lidar_raw_cam,camera_intrinsic,img_raw_size)\n",
        "lidar_2d1,lidar_in_cam_img1 = project_lidar2cam(lidar_raw_cam1,camera_intrinsic,img_raw_size)\n",
        "\n",
        "### Visualize\n",
        "print(f'Original image size: {img_raw.shape}')\n",
        "\n",
        "img = img_raw.copy()\n",
        "axs = 2\n",
        "axs_log = np.log(lidar_in_cam_img[:,axs]-np.min(lidar_in_cam_img[:,axs])+1)\n",
        "max_axs = np.max(axs_log)\n",
        "for pt,z in zip(lidar_2d,axs_log):\n",
        "    color_z = z*255/max_axs\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "img1 = img_raw1.copy()\n",
        "axs = 2\n",
        "axs_log1 = np.log(lidar_in_cam_img1[:,axs]-np.min(lidar_in_cam_img1[:,axs])+1)\n",
        "max_axs1 = np.max(axs_log1)\n",
        "for pt,z in zip(lidar_2d1,axs_log1):\n",
        "    color_z = z*255/max_axs1\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img1,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "fig,axs = plt.subplots(2,1,figsize=(15,7))\n",
        "axs[0].imshow(img)\n",
        "axs[1].imshow(img1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/project_lidar_camera.png\" width=1200px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "H7XM251OPhKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.2.a)"
      ],
      "metadata": {
        "id": "mS1HcPlbiJCD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTnCvzTGGO-"
      },
      "source": [
        "## Image Cropping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya3jzuZL7GzM"
      },
      "source": [
        "The deep learning model (DeepLab v3+) was trained with cropped images from KITTI dataset with ratio 4:3 (W:H), which was resized further to 513 x 513 images. It will work better if we use the same size of images.\n",
        "\n",
        "Parameter:\n",
        "- CROP_RH - The height ratio of target image size\n",
        "- CROP_RW - The width ratio of target image size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7aD86U97LyP"
      },
      "outputs": [],
      "source": [
        "### Function to crop the image according to the target size ratio\n",
        "### The image will be cropped at the center\n",
        "def crop_data(img_in,lidar_2d_in,lidar_in_cam_img,rh,rw):\n",
        "  lidar_2d = lidar_2d_in.copy()\n",
        "  lidar_in_cam_img = lidar_in_cam_img.copy()\n",
        "  img = img_in.copy()\n",
        "\n",
        "  ### Crop the image\n",
        "  dim_ori = np.array(img.shape)\n",
        "  cent = (dim_ori/2).astype(int)\n",
        "  if dim_ori[0]/dim_ori[1] == rh/rw:\n",
        "      crop_img = img\n",
        "\n",
        "  # If Height <= Width\n",
        "  elif dim_ori[0] <= dim_ori[1]:\n",
        "      cH2 = dim_ori[0]\n",
        "      cW2 = cH2*rw/rh\n",
        "      cW = int(cW2/2)\n",
        "      crop_img = img[:,cent[1]-cW:cent[1]+cW+1]\n",
        "\n",
        "  # If Height > Width\n",
        "  else:\n",
        "      cW2 = dim_ori[1]\n",
        "      cH2 = cW2*rh/rw\n",
        "      cH = int(cH2/2)\n",
        "      crop_img = img[cent[0]-cH:cent[0]+cH+1,:]\n",
        "\n",
        "  ### Filter out LiDAR points outside cropped image\n",
        "\n",
        "\n",
        "  cW = cW2/2 # horizontal size in pixels\n",
        "  cH = cH2/2 # vertical size in pixels\n",
        "  centH = cent[0] #center in horizontal\n",
        "  centW = cent[1] #center in vertical\n",
        "\n",
        "  maskH = #TODO (hint horizontal: lidar_2d[:,1])\n",
        "  maskW = #TODO (hint vertical: lidar_2d[:,0])\n",
        "  mask = np.logical_and(maskH,maskW)\n",
        "\n",
        "  lidar_2d_crop = lidar_2d[mask,:]\n",
        "  lidar_in_cam_img_crop = lidar_in_cam_img[mask,:]\n",
        "\n",
        "  cent = np.array((centW-cW,centH-cH,0)).reshape((1,3))\n",
        "  lidar_2d_crop = lidar_2d_crop - cent\n",
        "\n",
        "  return crop_img, lidar_2d_crop.astype(int), lidar_in_cam_img_crop\n",
        "\n",
        "### Cropped image's size ratio\n",
        "CROP_RH = 3 # Height ratio\n",
        "CROP_RW = 4 # Width ratio\n",
        "crop_img,lidar_2d_crop,lidar_in_cam_img_crop = crop_data(img_raw,lidar_2d,lidar_in_cam_img,CROP_RH,CROP_RW)\n",
        "crop_img1,lidar_2d_crop1,lidar_in_cam_img_crop1 = crop_data(img_raw1,lidar_2d1,lidar_in_cam_img1,CROP_RH,CROP_RW)\n",
        "\n",
        "### Visualize\n",
        "img = crop_img.copy()\n",
        "axs = 2\n",
        "axs_log = np.log(lidar_in_cam_img_crop[:,axs]-np.min(lidar_in_cam_img_crop[:,axs])+1)\n",
        "max_axs = np.max(axs_log)\n",
        "for pt,z in zip(lidar_2d_crop,axs_log):\n",
        "    color_z = z*255/max_axs\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "img1 = crop_img1.copy()\n",
        "#TODO (same as img above)\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,7))\n",
        "axs[0].imshow(img)\n",
        "axs[1].imshow(img1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/crop_lidar_camera.png\" width=400px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "a87q99H1PpvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.2.b)"
      ],
      "metadata": {
        "id": "XsKxIr_miQpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhy-xR-xKPu6"
      },
      "source": [
        "## Road Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxb-TFz0ATMJ"
      },
      "source": [
        "Process the camera image with image segmentation-based deep learning method to get regions that correspond to the road. Specifically, we use DeepLab v3+ model that has been trained before with KITTI dataset. The model has been trained on image with size 513 x 513. Thus, the input image need to be resized first before being processed by the model. [Read this article](https://rockyshikoku.medium.com/train-deeplab-v3-with-your-own-dataset-13f2af958a75) if you want to know how to train your own DeepLab v3+ model.\n",
        "\n",
        "Parameters:\n",
        "- DEEPLAB_MODEL_PATH - Path to the model protobuff (.pb) file\n",
        "- DEEPLAB_INPUT_SIZE - Input size of the model\n",
        "\n",
        "Now, by projecting the LiDAR points to the segmented image, we can know which points that correspond to the road."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM6rRV1h733l"
      },
      "outputs": [],
      "source": [
        "### Function to process the image with DeepLabv3+\n",
        "def process_images(img_in, sess, target_size=513, probability_threshold=0.5):\n",
        "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "  PROB_TENSOR_NAME = 'SemanticProbabilities:0'\n",
        "  INPUT_SIZE = target_size\n",
        "\n",
        "  image = img_in.copy()\n",
        "  sz = image.shape\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Resize input image to target size\n",
        "  if INPUT_SIZE == 0:\n",
        "    resized_image = image.copy()\n",
        "  else:\n",
        "    resized_image = cv2.resize(image,(INPUT_SIZE,INPUT_SIZE))\n",
        "\n",
        "  # Run deep learning inference\n",
        "  batch_seg_map = sess.run(\n",
        "      PROB_TENSOR_NAME,\n",
        "      feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "  seg_map = (batch_seg_map[0][:,:,1]*255).astype(int)\n",
        "  prob = np.array(seg_map, dtype=np.uint8)\n",
        "  prob = cv2.resize(prob,(sz[1],sz[0]))\n",
        "\n",
        "  # Create the prediction\n",
        "  pred = prob.copy()\n",
        "  msk_bin = prob >= (probability_threshold*255)\n",
        "  pred[msk_bin] = 1\n",
        "  pred[np.logical_not(msk_bin)] = 0\n",
        "\n",
        "  # Ignore regions that are separated from the main road\n",
        "  # This can reduce the amount of false detection\n",
        "  # hint- use cv2.connectedComponents\n",
        "  _,segm_reg = #TODO\n",
        "  segm_reg = segm_reg.astype(float)\n",
        "  segm_reg[segm_reg==0] = np.nan\n",
        "\n",
        "  modes,_ = stats.mode(segm_reg.flatten(),axis=None,nan_policy=\"omit\")\n",
        "  mode = modes[0]\n",
        "  pred[segm_reg!=mode] = 0\n",
        "\n",
        "  return prob,(pred*255).astype(np.uint8)\n",
        "\n",
        "### Load the model\n",
        "DEEPLAB_MODEL_PATH = 'course_ex1/pretrained/deeplab_model.pb'\n",
        "\n",
        "with open(DEEPLAB_MODEL_PATH, \"rb\") as f:\n",
        "    graph_def = tf.compat.v1.GraphDef.FromString(f.read())\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.import_graph_def(graph_def=graph_def, name=\"\")\n",
        "sess = tf.compat.v1.Session(graph=graph)\n",
        "\n",
        "DEEPLAB_INPUT_SIZE = 513\n",
        "segm_prob,segm_pred = process_images(crop_img, sess, DEEPLAB_INPUT_SIZE, 0.5)\n",
        "segm_prob1,segm_pred1 = process_images(crop_img1, sess, DEEPLAB_INPUT_SIZE, 0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfHpEwAeeLtE"
      },
      "outputs": [],
      "source": [
        "### Visualize\n",
        "segm_3ch = crop_img.copy()\n",
        "for pt in lidar_2d_crop:\n",
        "  pt = pt.astype(int)\n",
        "  if segm_pred[pt[1],pt[0]] == 0:\n",
        "    segm_3ch[pt[1],pt[0],0]=img[pt[1],pt[0],0]\n",
        "    segm_3ch[pt[1],pt[0],1]=img[pt[1],pt[0],1]\n",
        "    segm_3ch[pt[1],pt[0],2]=img[pt[1],pt[0],2]\n",
        "    continue\n",
        "  c = (255,0,0)\n",
        "  cv2.circle(segm_3ch,tuple(pt[:2]),1,c,-1)\n",
        "\n",
        "\n",
        "segm_3ch1 = crop_img1.copy()\n",
        "for pt in lidar_2d_crop:\n",
        "  pt = pt.astype(int)\n",
        "  if segm_pred1[pt[1],pt[0]] == 0:\n",
        "    continue\n",
        "  c = (255,0,0)\n",
        "  cv2.circle(segm_3ch1,tuple(pt[:2]),1,c,-1)\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,7))\n",
        "axs[0].imshow(segm_3ch)\n",
        "axs[1].imshow(segm_3ch1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/road_seg_image.png\" width=400px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "8T5oEWeTP1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dispaly road segmenation on image\n",
        "img_rgb = cv2.cvtColor(segm_pred,cv2.COLOR_GRAY2BGR)\n",
        "img_rgb[segm_pred>0,0]=0\n",
        "img_rgb[segm_pred>0,1]=20\n",
        "img_rgb[segm_pred>0,2]=255\n",
        "\n",
        "overlay_image =#TODO (hint-use  cv2.addWeighted function, crop img and img_rgb ) cv2.addWeighted(crop_img,0.9,img_rgb,0.8,1)\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(overlay_image,cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QCGTmugjkEtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.3)"
      ],
      "metadata": {
        "id": "VWmEygrqiaIr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5HB34TQc8bC"
      },
      "source": [
        "## LiDAR Road Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BBdmSkA5-4"
      },
      "source": [
        "The road points set that are obtained from the previous step may contain false detection and obviously only contains point that are inside the camera field of view. We also need to detect road points that are outside this set!\n",
        "\n",
        "To do this, fit a plane model (which will represent the road model) $Ax+By+Cz=1$ to the current road points. Any points that are outside the camera field of view and located near the road plane model can be also regarded as road points. RANSAC algorithm is applied when fitting the model to reduce the influence of outliers. Visit this [Wikipedia page](https://en.wikipedia.org/wiki/Random_sample_consensus) for more information about RANSAC.\n",
        "\n",
        "Parameter:\n",
        "- ROAD_HEIGHT_THRESHOLD - The maximum distance (in height axis) between a point to the road model to be considered as road point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Get the plane model from the road points\n",
        "def get_road_model_ransac(img_pred,lidar_in_cam_img_crop,lidar_2d_crop):\n",
        "\n",
        "  #inputs: lidar_in_cam_img_crop- points cloud on the image in image coordinates\n",
        "\n",
        "  #using the image and lidar2d to check which point cloud is on the road!\n",
        "  lidar_in_road_lbl = [True if img_pred[pt[1],pt[0]] == 255 else False for pt in lidar_2d]\n",
        "\n",
        "\n",
        "  # filter the road points from camera coordinate space!, the model is based on camera coordinate!\n",
        "  lidar_in_road = lidar_in_cam_img_crop[lidar_in_road_lbl,:]\n",
        "\n",
        "  Mininal_set_of_point=10; #optional, you can calibrate this value\n",
        "\n",
        "  if len(lidar_in_road[:,[0,2]])<Mininal_set_of_point:\n",
        "     #In the case of few points, there are not enough statistics so we simply use the previous model\n",
        "    status=0\n",
        "    road_model=None\n",
        "  else:\n",
        "    # Estimate the road model\n",
        "    # (hint- use RANSACRegressor function, lidar_in_road and Mininal_set_of_point )\n",
        "    # TODO\n",
        "    status=1\n",
        "\n",
        "  #outputs: road_model and status (0- there is no model! ,1- new model)\n",
        "\n",
        "  return road_model,status"
      ],
      "metadata": {
        "id": "tmblGaWOibbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKocWSZ9dG-D"
      },
      "outputs": [],
      "source": [
        "### Predict all LiDAR points as road or not\n",
        "def filter_road_points(road_model,lidar_raw_cam,threshold=0.5):\n",
        "  #inputs: trained road model by RANSACRegressor, lidar raw data in camera coordinates and threshould of road height(m)\n",
        "\n",
        "  x = # TODO (hint-point cloud:  x and z axes )\n",
        "  y_true = # TODO (hint-point cloud:  y axes )\n",
        "  y_pred = # TODO (hint-model predict )\n",
        "  delta_y =#TODO hint (absolute(y_true-y_pred))   np.absolute(y_true-y_pred).flatten()\n",
        "  is_not_road =#TODO (hint- use threshold)\n",
        "  lidar_out = #TODO (point cloud of the road)\n",
        "  lidar_road = #TODO (point cloud of the rest objects)\n",
        "  return lidar_out,lidar_road\n",
        "\n",
        "road_height_threshold = 0.3 #optional, you can calibrate this value\n",
        "\n",
        "road_model,status = get_road_model_ransac(segm_pred,lidar_in_cam_img_crop,lidar_2d_crop)\n",
        "lidar_nonroad,lidar_road = filter_road_points(road_model,lidar_raw_cam,road_height_threshold)\n",
        "\n",
        "road_model1,status1 = get_road_model_ransac(segm_pred1,lidar_in_cam_img_crop1,lidar_2d1)\n",
        "lidar_nonroad1,lidar_road1 = filter_road_points(road_model1,lidar_raw_cam1,road_height_threshold)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(20,10))\n",
        "axs[0].scatter(lidar_nonroad[:,0],lidar_nonroad[:,2],c=-lidar_nonroad[:,1],marker='.')\n",
        "axs[0].scatter(lidar_road[:,0],lidar_road[:,2],c='b',marker='.')\n",
        "axs[0].scatter(0,0,c='r',marker='x')\n",
        "axs[0].axis('scaled')\n",
        "axs[1].scatter(lidar_nonroad1[:,0],lidar_nonroad1[:,2],c=-lidar_nonroad1[:,1],marker='.')\n",
        "axs[1].scatter(lidar_road1[:,0],lidar_road1[:,2],c='b',marker='.')\n",
        "axs[1].scatter(0,0,c='r',marker='x')\n",
        "axs[1].axis('scaled')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJENqREB-xe"
      },
      "source": [
        "Now we can remove the road points from the LiDAR data and proceed to the mapping system."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C.4)"
      ],
      "metadata": {
        "id": "oNB-h9ZnidAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robot mapping based on Deep Learning!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4iZgoKlZjNFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYIDSq_0uaKj"
      },
      "outputs": [],
      "source": [
        "#Dispaly road segmenation on image\n",
        "img_rgb = cv2.cvtColor(segm_pred,cv2.COLOR_GRAY2BGR)\n",
        "img_rgb[segm_pred>0,0]=0\n",
        "img_rgb[segm_pred>0,1]=20\n",
        "img_rgb[segm_pred>0,2]=255\n",
        "\n",
        "overlay_image =#TODO (hint-use  cv2.addWeighted function, crop img and img_rgb ) cv2.addWeighted(crop_img,0.9,img_rgb,0.8,1)\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(overlay_image,cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing"
      ],
      "metadata": {
        "id": "qc0RaY5UQXSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-Processing (transform T_cam2_imu)\n",
        "camera_to_imu_calibration = np.linalg.inv(data.calib.T_cam2_imu)"
      ],
      "metadata": {
        "id": "AzYu6WRJT8VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize OGM\n",
        "Occupancy_map = np.ones((MAP_SIZE_Y,MAP_SIZE_X)) * 0.5\n"
      ],
      "metadata": {
        "id": "s_XA8QiC_dce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(data.velo_files)"
      ],
      "metadata": {
        "id": "mIlS6fomasPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26aPwSM8ZfcK"
      },
      "outputs": [],
      "source": [
        "##Initialize occupancy grid\n",
        "Occupancy_map =#TODO\n",
        "\n",
        "DEEPLAB_MODEL_PATH = 'KITTI_Mapping/pretrained/deeplab_model.pb'\n",
        "with open(DEEPLAB_MODEL_PATH, \"rb\") as f:\n",
        "    graph_def = tf.compat.v1.GraphDef.FromString(f.read())\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.import_graph_def(graph_def=graph_def, name=\"\")\n",
        "sess = tf.compat.v1.Session(graph=graph)\n",
        "\n",
        "\n",
        "for idx_frame in range(N):\n",
        "\n",
        "\n",
        "  if (idx_frame ==0):\n",
        "    pose_prev = (0,0,0)\n",
        "    prev_indx=idx_frame\n",
        "  else:\n",
        "\n",
        "    prev_indx=idx_frame-1\n",
        "\n",
        "  ##1. Pre processing: sensor Fusion!! Lidar-camera-imu, great fun!\n",
        "\n",
        "  #load data (hint- use \"load_data\" )\n",
        "  img_raw,lidar_raw = #TODO\n",
        "\n",
        "  #filter Lidar below sensor (hint- use \"filter Lidar below sensor\" )\n",
        "  lidar_raw_below=#TODO\n",
        "\n",
        "  #Transform lidar to camera coordinates (hint- use \"transform_coordinate\")\n",
        "  lidar_raw_cam = #TODO\n",
        "\n",
        "  #project the point cloud from camera coordinates to image coordiantes (hint- use \"project_lidar2cam\")\n",
        "  lidar_2d,lidar_in_cam =#TODO\n",
        "\n",
        "  CROP_RH = 3 # Height ratio\n",
        "  CROP_RW = 4 # Width ratio\n",
        "\n",
        "  #Crop the point cloud (use \"crop_data\")\n",
        "  crop_img,lidar_2d_crop,lidar_in_cam_img_crop = #TODO\n",
        "\n",
        "  #2. Run deepLabv3+! (use \"process_images\") , amazing!\n",
        "  # prms:crop_img, sess, DEEPLAB_INPUT_SIZE, 0.5\n",
        "  DEEPLAB_INPUT_SIZE = 513\n",
        "  segm_prob,segm_pred = #TODO\n",
        "\n",
        "  road_height_threshold = 0.08 #optional\n",
        "  #train road model based on RANSAC (use \"get_road_model_ransac\")\n",
        "  road_model,status = #TODO\n",
        "  if status==0:\n",
        "    road_model=prev_road_model\n",
        "  else:\n",
        "    prev_road_model=road_model\n",
        "\n",
        "  #Filter the road from all the point cloud (use  \"filter_road_point\")\n",
        "  lidar_nonroad,lidar_road = filter_road_points(road_model,lidar_raw,road_height_threshold)\n",
        "\n",
        "  # transform the point cloud w/o the road to IMU (use \"camera_to_imu_calibration\")\n",
        "  lidar_ogm= np.concatenate((lidar_nonroad,np.ones((lidar_nonroad.shape[0],1))),axis=1)\n",
        "  lidar_no_road_IMU = #TODO\n",
        "\n",
        "  #Occupancy map flow\n",
        "  #pose = #TODO         (hint- use load_vehicle_vel )\n",
        "  #shifted_ogm =#TODO   (hint- use shift_pose_ogm)\n",
        "  #ogm_step =#TODO      (hint- generate_measurement_ogm use \"lidar_no_road_IMU\")\n",
        "  #Occupancy_map =#TODO (hint- update_ogm)\n",
        "\n",
        "\n",
        "  pose_prev=pose\n",
        "\n",
        "  img_rgb = cv2.cvtColor(segm_pred,cv2.COLOR_GRAY2BGR)\n",
        "  img_rgb[segm_pred>0,0]=0\n",
        "  img_rgb[segm_pred>0,1]=20\n",
        "  img_rgb[segm_pred>0,2]=255\n",
        "\n",
        "  overlay_image =#TODO: (use cv2.addWeighted, crop_img and img_rgb)\n",
        "\n",
        "  fig,axs = plt.subplots(2,2,figsize=(15,12))\n",
        "  fig1 = plt.gcf()\n",
        "  axs[0,0].imshow(overlay_image,cmap='gray')\n",
        "  axs[0,0].set_title(\"Road segmenation At Frame Number={:03}\".format(idx_frame))\n",
        "  axs[0,0].axis('scaled')\n",
        "  axs[1,1].imshow(Occupancy_map)\n",
        "  axs[1,1].scatter(500,500,c='r',marker='x')\n",
        "  axs[1,1].axis('scaled')\n",
        "  axs[1,1].set_title(\"Occupancy Map\")\n",
        "  axs[1,0].imshow(ogm_step)\n",
        "  axs[1,0].scatter(500,500,c='r',marker='x')\n",
        "  axs[1,0].set_title(\"Hitmap- scan grid\")\n",
        "  #axs[0,1].scatter(-lidar_raw[:,1],lidar_raw[:,0],c=lidar_raw[:,2],marker='.')\n",
        "  axs[0,1].scatter(0,0,c='r',marker='x')\n",
        "  axs[0,1].scatter(lidar_nonroad[:,0],lidar_nonroad[:,2],c=-lidar_nonroad[:,1],marker='.')\n",
        "  axs[0,1].scatter(lidar_road[:,0],lidar_road[:,2],c='b',marker='.')\n",
        "  axs[0,1].scatter(0,0,c='r',marker='x')\n",
        "  axs[0,1].set_xlim(-80,80)\n",
        "  axs[0,1].set_ylim(-80,80)\n",
        "  axs[0,1].axis('scaled')\n",
        "\n",
        "  #axs[0,1].axis('scaled')\n",
        "  axs[0,1].set_title(\"Road segmenation based on Deep Learning\")\n",
        "  plt.show()\n",
        "  plt.draw()\n",
        "  fig1.savefig(\"%s/occupancy_map_frame_{:03}.png\".format(idx_frame) % '/content/gdrive/MyDrive/Results/', dpi=100 )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/orfaig/course_ex1/146218cc7ffabae80fe546b68ba03a6073fbefa2/figures/road_occ_frame_3.png\" width=1200px></center>\n",
        "</br>"
      ],
      "metadata": {
        "id": "c1524wWlQxQd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "tp3269Uu303u",
        "iderI-dPJ7QJ",
        "EZXtOnG_ZOX7",
        "ZbncWyCNjbSu",
        "iAa7YBNcjuGn",
        "9VQdc8VUYnEw",
        "wOHqk_PjbLEA",
        "kgQlBTCV4MEN",
        "RcVJSGd6d66M",
        "KkSEoPVNzmh8",
        "a4L5_nHBxw24",
        "cDiDJTa7J-fB",
        "uVQYbI6wAbdv",
        "y2sVVXDiAfw5",
        "3QTNtHYfCs2e",
        "1aTnCvzTGGO-",
        "zhy-xR-xKPu6",
        "k5HB34TQc8bC",
        "4iZgoKlZjNFH",
        "qc0RaY5UQXSR"
      ],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "4147159148f2331f27fc35f83760ac3303b0eb37178f521715f234bfc6af8027"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}